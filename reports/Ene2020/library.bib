Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Fatih2016,
abstract = {The usage areas of biometric systems are becoming widespread in today's technology. Face recognition systems among biometric systems; Ease of use, reliability, cost, etc., the preference between public institutions, commercial enterprises and researchers is increasing. In this study, it is suggested that students should use face recognition system instead of traditional methods of absenteeism in education and training institutions. It is very important that face recognition systems work quickly with matching people correctly. In this study, the training and recognition times of Eigenfaces, Fisherfaces and Local Binary Pattern algorithms used in face recognition systems are calculated by using Visual C ++ and Python programming languages using ORL dataset. Keywords},
author = {Fatih, Ä°lkbahar and Resul, Kara},
file = {:C$\backslash$:/Users/uzmar/OneDrive/Documentos/Mendeley Desktop/Fatih, Resul - 2017 - Performance Analysis of Face Recognition Algorithms.pdf:pdf},
isbn = {978-1-5386-1880-6},
keywords = {Face recognition,performance analysis,video processing},
title = {{Performance Analysis of Face Recognition Algorithms}},
year = {2017}
}
@article{Liton2012b,
abstract = {This paper mainly addresses the building of face recognition system by using Principal Component Analysis (PCA). PCA is a statistical approach used for reducing the number of variables in face recognition. In PCA, every image in the training set is represented as a linear combination of weighted eigenvectors called eigenfaces. These eigenvectors are obtained from covariance matrix of a training image set. The weights are found out after selecting a set of most relevant Eigenfaces. Recognition is performed by projecting a test image onto the subspace spanned by the eigenfaces and then classification is done by measuring minimum Euclidean distance. A number of experiments were done to evaluate the performance of the face recognition system. In this thesis, we used a training database of students of Electronics and Telecommunication Engineering department, Batch-2007, Rajshahi University of Engineering and Technology, Bangladesh.},
author = {Liton, Paul and Abdulla, Summan},
file = {:C$\backslash$:/Users/uzmar/OneDrive/Documentos/Mendeley Desktop/Liton, Abdulla - 2012 - Face Recognition Using Principal Component Analysis Method.pdf:pdf},
issn = {22781323},
journal = {International Journal of Advancements in Computing Technology},
keywords = {Covariance,Eigenface,Eigenvalue,Eigenvector,Euclidean distance,PCA},
number = {1},
title = {{Face Recognition Using Principal Component Analysis Method}},
volume = {9},
year = {2012}
}
@article{Tun2019b,
abstract = {In the information era, the size of network traffic is complex because of massive Internet-based services and rapid amounts of data. The more network traffic has enhanced, the more cyberattacks have dramatically increased. Therefore, cybersecurity intrusion detection has been a challenge in the current research area in recent years. The Intrusion detection system requires high-level protection and detects modern and complex attacks with more accuracy. Nowadays, big data analytics is the main key to solve marketing, security and privacy in an extremely competitive financial market and government. If a huge amount of stream data flows within a short period time, it is difficult to analyze real-time decision making. Performance analysis is extremely important for administrators and developers to avoid bottlenecks. The paper aims to reduce time-consuming by using Apache Kafka and Spark Streaming. Experiments on the UNSWNB-15 dataset indicate that the integration of Apache Kafka and Spark Streaming can perform better in terms of processing time and fault-tolerance on the huge amount of data. According to the results, the fault tolerance can be provided by the multiple brokers of Kafka and parallel recovery of Spark Streaming. And then, the multiple partitions of Apache Kafka increase the processing time in the integration of Apache Kafka and Spark Streaming.},
author = {Tun, May Thet and Nyaung, Dim En and Phyu, Myat Pwint},
doi = {10.1109/AITC.2019.8920960},
file = {:C$\backslash$:/Users/uzmar/OneDrive/Documentos/Mendeley Desktop/Tun, Nyaung, Phyu - 2019 - Performance Evaluation of Intrusion Detection Streaming Transactions Using Apache Kafka and Spark Streaming.pdf:pdf},
isbn = {9781728151731},
journal = {2019 International Conference on Advanced Information Technologies, ICAIT 2019},
keywords = {Apache Kafka,Apache Spark Streaming,Big Data,Intrusion Detection},
pages = {25--30},
publisher = {IEEE},
title = {{Performance Evaluation of Intrusion Detection Streaming Transactions Using Apache Kafka and Spark Streaming}},
year = {2019}
}
@article{Shree2018b,
abstract = {In todays 21st century as technology is getting so much advanced, Apache Kafka emanate as one of the finest technology in the present world. Its fast, scalable, distributed stream processing platform and fault tolerant messaging system has made this technology to roar in the field of data processing and analysis. Apache Kafka is a distributed streaming platform mainly designed for low latency and high throughput. It is publish-subscribe messaging reassess as a constituent to each of number of legatee of commit log. The key notion of Apache Kafka is that it is used as a cluster on any number of servers. Server of Kafka stores record streams in classes known as topics. Every record contains a key, a value and a time stamp. It has two classes of application. Firstly, for building pipelines of real time data streams which is reliable to get the data between the systems or between the applications. Secondly, build applications streaming for real time that reacts to the record streams. A single Kafka mediator can handle hundreds of megabytes of reads and writes per second from thousands of clients. Scalable Kafka is designed to allow a single cluster to serve the central data backbone for a large organization.},
author = {Shree, Rishika and Choudhury, Tanupriya and Gupta, Subhash Chand and Kumar, Praveen},
doi = {10.1109/TEL-NET.2017.8343593},
file = {:C$\backslash$:/Users/uzmar/OneDrive/Documentos/Mendeley Desktop/Shree et al. - 2018 - KAFKA The modern platform for data management and analysis in big data domain.pdf:pdf},
isbn = {9781509067107},
journal = {2nd International Conference on Telecommunication and Networks, TEL-NET 2017},
keywords = {Apache Kafka,consumer,flume,hadoop,high throughput,low latency,producer,publish-subscribe,stream processing},
pages = {1--5},
title = {{KAFKA: The modern platform for data management and analysis in big data domain}},
volume = {2018-Janua},
year = {2018}
}
@article{Singh2017b,
abstract = {Disguised face identification (DFI) is an extremely challenging problem due to the numerous variations that can be introduced using different disguises. This paper introduces a deep learning framework to first detect 14 facial key-points which are then utilized to perform disguised face identification. Since the training of deep learning architectures relies on large annotated datasets, two annotated facial key-points datasets are introduced. The effectiveness of the facial keypoint detection framework is presented for each keypoint. The superiority of the key-point detection framework is also demonstrated by a comparison with other deep networks. The effectiveness of classification performance is also demonstrated by comparison with the state-of-the-art face disguise classification methods.},
archivePrefix = {arXiv},
arxivId = {1708.09317},
author = {Singh, Amarjot and Patii, Devendra and Reddy, G. Meghana and Omkar, S. N.},
doi = {10.1109/ICCVW.2017.193},
eprint = {1708.09317},
file = {:C$\backslash$:/Users/uzmar/OneDrive/Documentos/Mendeley Desktop/Singh et al. - 2017 - Disguised Face Identification (DFI) with Facial KeyPoints Using Spatial Fusion Convolutional Network.pdf:pdf},
isbn = {9781538610343},
journal = {Proceedings - 2017 IEEE International Conference on Computer Vision Workshops, ICCVW 2017},
pages = {1648--1655},
title = {{Disguised Face Identification (DFI) with Facial KeyPoints Using Spatial Fusion Convolutional Network}},
volume = {2018-Janua},
year = {2017}
}
@article{Susu2016,
abstract = {We present an efficient open-source implementation of a novel video alignment algorithm, based on low dimensionality frame matching and the recently introduced ECC image registration algorithm.},
author = {Susu, Alexandru E. and Codreanu, Valeriu and Evangelidis, Georgios and Petrica, Lucian},
doi = {10.1109/ICComm.2016.7528343},
file = {:C$\backslash$:/Users/uzmar/OneDrive/Documentos/Mendeley Desktop/Susu et al. - 2016 - Efficient implementation of a video change detection algorithm.pdf:pdf},
isbn = {9781467381963},
issn = {15503607},
journal = {IEEE International Conference on Communications},
keywords = {C++,CMP,GPU,Matlab,OpenCV,Python,embedded systems,kd-tree,video alignment},
pages = {77--82},
title = {{Efficient implementation of a video change detection algorithm}},
volume = {2016-Augus},
year = {2016}
}
@article{Yang2017b,
abstract = {Considering the complexity of data analysis caused by increasing urban load data, an Apache Spark based urban load analysis and forecasting technology is proposed in this paper. Firstly, the large-scale data processing platform is designed. Secondly, statistics of urban load data are processed by transformation and action in the form of resilient distributed datasets (RDDs). Thirdly, load forecasting model based on Dynamic Bayesian Network (DBN) is built for short-term urban load forecasting. GraphX is used for graph-parallel computation. Finally, the result of load analysis is presented and forecasting results of DBN are compared with other models. The comparison outcome shows that the DBN model implemented in Spark has better accuracy in load forecasting.},
author = {Yang, Chenggang and Song, Yan and Qian, Jiang and Zhao, Hanying and Jiang, Wei and Tang, Haibo and Wu, Jie},
doi = {10.1109/EI2.2017.8245396},
file = {:C$\backslash$:/Users/uzmar/OneDrive/Documentos/Mendeley Desktop/Yang et al. - 2017 - Apache spark based urban load data analysis and forecasting technology research.pdf:pdf},
isbn = {9781538614273},
journal = {2017 IEEE Conference on Energy Internet and Energy System Integration, EI2 2017 - Proceedings},
keywords = {Dynamic Bayesian network,Spark,load analysis and forecasting},
pages = {1--6},
title = {{Apache spark based urban load data analysis and forecasting technology research}},
volume = {2018-Janua},
year = {2017}
}
@article{Xing2018b,
abstract = {Face verification is a task to determine whether a pair of given facial images belong to the same person. In unconstrained real applications, inter and intra variations, including illumination, pose, occlusion, and expression, will seriously decrease the verification performance. Due to the lack of annotated data for face verification, extended datasets for face recognition with large samples are used to assist learning a robust feature representation generally. However, the extended data for face recognition is different from face verification on distribution and task. In this paper, a transfer learning based on PCA-SVM is proposed to alleviate above problem. The original feature representation is learnt from a deep convolutional neural network by face classification. Then a PCA-SVM based transfer method is used for feature reprojection from the source domain (face recognition) to the target domain (face verification), which reduces the divergence of feature distribution and task inconsistency. The proposed framework yields comparable results and the accuracy is 98.5{\%} on LFW dataset.},
author = {Xing, Xiaofen and Xu, Guicong and Cai, Bolun and Qing, Chunmei and Xu, Xiangmin},
doi = {10.1109/iThings-GreenCom-CPSCom-SmartData.2017.166},
file = {:C$\backslash$:/Users/uzmar/OneDrive/Documentos/Mendeley Desktop/Xing et al. - 2018 - Face verification based on feature transfer via PCA-SVM framework.pdf:pdf},
isbn = {9781538630655},
journal = {Proceedings - 2017 IEEE International Conference on Internet of Things, IEEE Green Computing and Communications, IEEE Cyber, Physical and Social Computing, IEEE Smart Data, iThings-GreenCom-CPSCom-SmartData 2017},
pages = {1086--1091},
title = {{Face verification based on feature transfer via PCA-SVM framework}},
volume = {2018-Janua},
year = {2018}
}
@article{Khan2018b,
abstract = {Social networks generate enormous amounts of visual data. Mining of such data in recommender systems is extremely important. User profiling is carried out in recommender systems to build the holistic persona of the user. Identification and grouping of images in these systems is carried out using face recognition. It is one of the most appropriate biometric features in such situations. Ever since the first use of face recognition in security and surveillance systems, researchers have developed many methods with improved accuracy. Face recognition under variant illumination is still an open issue and diverging facial expressions reduces the accuracy even further. State of the art methods produced an average accuracy of 90{\%}.In this study, a computationally intelligent and efficient method based on particle swarm optimization (PSO) is developed. It utilizes the features extracted from texture and wavelet domain. Discrete Wavelet Transform provides the advantage of extracting relevant features and thereby reducing computational time and an increase in recognition accuracy rate. We apply particle swarm optimization technique to select informative wavelet sub-band. Furthermore, the proposed technique uses Discrete Fourier Transform to compensate the translational variance problem of the discrete wavelet transform. The proposed method has been tested on the CK, MMI and JAFFE databases. Experimental results are compared with existing techniques and the results indicate that the proposed technique is more robust to illumination and variation in expressions, average accuracy obtained over the CK, MMI and JAFFE datasets is 98.6{\%}, 95.5{\%}, and 98.8{\%} respectively.},
author = {Khan, Sajid Ali and Ishtiaq, Muhammad and Nazir, Muhammad and Shaheen, Muhammad},
doi = {10.1016/j.jocs.2018.08.005},
file = {:C$\backslash$:/Users/uzmar/OneDrive/Documentos/Mendeley Desktop/Khan et al. - 2018 - Face recognition under varying expressions and illumination using particle swarm optimization.pdf:pdf},
issn = {18777503},
journal = {Journal of Computational Science},
keywords = {Face expressions,Face recognition,Local binary pattern,Particle swarm optimization,Variant illumination,Wavelets},
pages = {94--100},
publisher = {Elsevier B.V.},
title = {{Face recognition under varying expressions and illumination using particle swarm optimization}},
url = {https://doi.org/10.1016/j.jocs.2018.08.005},
volume = {28},
year = {2018}
}
@article{Elmahmudi2019b,
abstract = {Today, computer based face recognition is a mature and reliable mechanism which is being practically utilised for many access control scenarios. As such, face recognition or authentication is predominantly performed using âperfect' data of full frontal facial images. Though that may be the case, in reality, there are numerous situations where full frontal faces may not be available â the imperfect face images that often come from CCTV cameras do demonstrate the case in point. Hence, the problem of computer based face recognition using partial facial data as probes is still largely an unexplored area of research. Given that humans and computers perform face recognition and authentication inherently differently, it must be interesting as well as intriguing to understand how a computer favours various parts of the face when presented to the challenges of face recognition. In this work, we explore the question that surrounds the idea of face recognition using partial facial data. We explore it by applying novel experiments to test the performance of machine learning using partial faces and other manipulations on face images such as rotation and zooming, which we use as training and recognition cues. In particular, we study the rate of recognition subject to the various parts of the face such as the eyes, mouth, nose and the cheek. We also study the effect of face recognition subject to facial rotation as well as the effect of recognition subject to zooming out of the facial images. Our experiments are based on using the state of the art convolutional neural network based architecture along with the pre-trained VGG-Face model through which we extract features for machine learning. We then use two classifiers namely the cosine similarity and the linear support vector machines to test the recognition rates. We ran our experiments on two publicly available datasets namely, the controlled Brazilian FEI and the uncontrolled LFW dataset. Our results show that individual parts of the face such as the eyes, nose and the cheeks have low recognition rates though the rate of recognition quickly goes up when individual parts of the face in combined form are presented as probes.},
author = {Elmahmudi, Ali and Ugail, Hassan},
doi = {10.1016/j.future.2019.04.025},
file = {:C$\backslash$:/Users/uzmar/OneDrive/Documentos/Mendeley Desktop/Elmahmudi, Ugail - 2019 - Deep face recognition using imperfect facial data.pdf:pdf},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Convolutional neural networks,Cosine similarity,Deep learning,Face recognition},
number = {May},
pages = {213--225},
publisher = {Elsevier B.V.},
title = {{Deep face recognition using imperfect facial data}},
url = {https://doi.org/10.1016/j.future.2019.04.025},
volume = {99},
year = {2019}
}
@article{Linge2014b,
abstract = {Human face is contexture multidimensional point of vision model and by creating computational model for human face recognition is too hard. The paper present two methodologies for the face recognition, the first one is feature extraction and second is the feed forward back propagation neural network. The feature extraction is with Principal Component Analysis and classification with the help of neural network. Eigenfaces are applied to taken out the remedial information in an image, which are persistent for identification. For image recognition the Eigen face approach uses Principal Component Analysis (PCA) algorithm. The proposed algorithm has been tested on 165 images from Yale face database. Test results gave a recognition rate above the 97{\%}.},
author = {Linge, Ganesh and Pawar, Meenakshi},
file = {:C$\backslash$:/Users/uzmar/OneDrive/Documentos/Mendeley Desktop/Linge, Pawar - 2014 - Neural Network Based Face Recognition Using PCA.pdf:pdf},
issn = {0975-9646},
journal = {International Journal of Computer Science and Information Technologies},
keywords = {Artificial Neural Network,Eigenface,Face    Recognition,MATLAB,Principal    Component Analysis},
number = {3},
pages = {4011--4014},
title = {{Neural Network Based Face Recognition Using PCA}},
url = {http://ijcsit.com/docs/Volume 5/vol5issue03/ijcsit20140503299.pdf},
volume = {5},
year = {2014}
}
@article{Wang2012b,
abstract = {Face detect application has a real time need in nature. Although Viola-Jones algorithm can handle it elegantly, today's bigger and bigger high quality images and videos still bring in the new challenge of real time needs. It is a good idea to parallel the Viola-Jones algorithm with OpenCL to achieve high performance across both AMD and NVidia GPU platforms without bringing up new algorithms. This paper presents the bottleneck of this application and discusses how to optimize the face detection step by step from a very nave implementation. Some brilliant tricks and methods like CPU execution time hidden, stubbles usage of local memory as high speed scratchpad and manual cache, and variable granularity were used to improve the performance. Those technologies result in 4-13 times speedup varying with the image size. Furthermore, those ideas may throw on some light on the way to parallel applications efficiently with OpenCL. Taking face detection as an example, this paper also summarizes some universal advice on how to optimize OpenCL program, trying to help other applications do better on GPU.},
author = {Wang, Weiyan and Zhang, Yunquan and Yan, Shengen and Zhang, Ying and Jia, Haipeng},
doi = {10.1109/TST.2012.6216758},
file = {:C$\backslash$:/Users/uzmar/OneDrive/Documentos/Mendeley Desktop/Wang et al. - 2012 - Parallelization and performance optimization on face detection algorithm with OpenCL A case study.pdf:pdf},
issn = {10070214},
journal = {Tsinghua Science and Technology},
keywords = {OpenCL,Viola-Jones,local memory usage,parallel granularity,time cost hidden},
number = {3},
pages = {287--295},
title = {{Parallelization and performance optimization on face detection algorithm with OpenCL: A case study}},
volume = {17},
year = {2012}
}
@article{Pandey2018b,
abstract = {Face recognition of pedestrians by analyzing live video streaming, aims to identify movements and faces by performing image matching with existing images using Apache Spark Streaming, Kafka and OpenCV, on distributed platform and derive decisions. Since video processing and analysis from multiple resources become slow when using Cloud or even any single highly configured machine, hence for making quick decisions and actions, Apache Spark Streaming and Kafka have been used as real time analysis frameworks, which deliver event based decisions making on Hadoop distributed environment. If continuous live events analysis is possible then the decision can make there-after or at the same time. And large amount videos in parallel processing are also not a bottleneck after getting the involvement of Hadoop because base of all real time analysis distributed tools is Hadoop. This event based analysis can be applied at any place where an immediate action is required like monitoring border areas of countries by cameras and drones, road traffic monitoring, life science domain, airlines, logo recognition and where-ever continuous monitoring and decision making involved in large scale data set.},
author = {Pandey, Abhinav and Singh, Harendra},
file = {:C$\backslash$:/Users/uzmar/OneDrive/Documentos/Mendeley Desktop/Pandey, Singh - 2018 - Face recognition of pedestrians from live video stream using apache spark streaming and Kafka.pdf:pdf},
issn = {22783075},
journal = {International Journal of Innovative Technology and Exploring Engineering},
keywords = {Distributed system,Hadoop,Kafka,Open CV,Spark,Spark streaming},
number = {5},
pages = {4--10},
title = {{Face recognition of pedestrians from live video stream using apache spark streaming and Kafka}},
volume = {7},
year = {2018}
}
@article{Li2016b,
abstract = {Robust face recognition is an active topic in computer vision, while face occlusion is one of the most challenging problems for robust face recognition algorithm. The latest research on low-rank representation demonstrated its high efficiency to subspace segmentation and feature extraction. Motivated by previous work, in this paper, we consider the problem of human face recognition from frontal views with varying illumination, as well as occlusion and disguise. We present a novel approach for face recognition by extracting dynamic subspace of images and obtaining the discriminative parts in each individual. We use these parts to represent the characteristic of discriminative components, give a recognition protocol to classify face images. The experiments carried on publicly available databases (i.e., AR, Extended Yale B, and ORL) vilidate its accuray, robustness and speed. The proposed method needs lower dimensions training samples but gains a higher recognition rate than other popular approaches.},
author = {Li, Hongjun and Suen, Ching Y.},
doi = {10.1016/j.patcog.2016.05.014},
file = {:C$\backslash$:/Users/uzmar/OneDrive/Documentos/Mendeley Desktop/Li, Suen - 2016 - Robust face recognition based on dynamic rank representation.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Discriminative component,Dynamic subspace,Face recognition,Low-rank representation,Occlusion},
pages = {13--24},
publisher = {Elsevier},
title = {{Robust face recognition based on dynamic rank representation}},
url = {http://dx.doi.org/10.1016/j.patcog.2016.05.014},
volume = {60},
year = {2016}
}
@article{Cai2019b,
abstract = {With the superiority of three-dimensional (3D) scanning data, e.g., illumination invariance and pose robustness, 3D face recognition theoretically has the potential to achieve better results than two-dimensional (2D) face recognition. However, traditional 3D face recognition techniques suffer from high computational costs. This paper proposes a fast and robust 3D face recognition approach with three component technologies: a fast 3D scan preprocessing, multiple data augmentation, and a deep learning technique based on facial component patches. First, unlike the majority of the existing approaches, which require accurate facial registration, the proposed approach uses only three facial landmarks. Second, the specifical deep network with an improved supervision is designed to extract complementary features from four overlapping facial component patches. Finally, a data augmentation technique and three self-collected 3D face datasets are used to enlarge the scale of the training data. The proposed approach outperforms the state-of-the-art algorithms on four public 3D face benchmarks, i.e., 100{\%}, 99.75{\%}, 99.88{\%}, and 99.07{\%} rank-1 IRs with the standard test protocol on the FRGC v2.0, Bosphorus, BU-3DFE, and 3D-TEC datasets, respectively. Further, it requires only 0.84 seconds to identify a probe from a gallery with 466 faces.},
author = {Cai, Ying and Lei, Yinjie and Yang, Menglong and You, Zhisheng and Shan, Shiguang},
doi = {10.1016/j.neucom.2019.07.047},
file = {:C$\backslash$:/Users/uzmar/OneDrive/Documentos/Mendeley Desktop/Cai et al. - 2019 - A fast and robust 3D face recognition approach based on deeply learned face representation.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {3D face recognition,Deep learning,Face preprocessing,Multiple data augmentation},
pages = {375--397},
publisher = {Elsevier B.V.},
title = {{A fast and robust 3D face recognition approach based on deeply learned face representation}},
volume = {363},
year = {2019}
}
@article{LeNoach2017b,
abstract = {Stream computing is becoming a more and more popular paradigm as it enables the real-time promise of data analytics. Apache Kafka is currently the most popular framework used to ingest the data streams into the processing platforms. However, how to tune Kafka and how much resources to allocate for it remains a challenge for most users, who now rely mainly on empirical approaches to determine the best parameter settings for their deployments. In this poster, we make a through evaluation of several configurations and performance metrics of Kafka in order to allow users avoid bottlenecks, reach its full potential and avoid bottlenecks and eventually leverage some good practice for efficient stream processing.},
author = {{Le Noac'h}, Paul and Costan, Alexandru and Boug{\'{e}}, Luc},
doi = {10.1109/BigData.2017.8258548},
file = {:C$\backslash$:/Users/uzmar/OneDrive/Documentos/Mendeley Desktop/Le Noac'h, Costan, Boug{\'{e}} - 2017 - A performance evaluation of Apache Kafka in support of big data streaming applications.pdf:pdf},
isbn = {9781538627143},
journal = {Proceedings - 2017 IEEE International Conference on Big Data, Big Data 2017},
keywords = {Apache Kafka,Big Data,Stream computing},
pages = {4803--4806},
title = {{A performance evaluation of Apache Kafka in support of big data streaming applications}},
volume = {2018-Janua},
year = {2017}
}
@article{Chintapalli2016b,
abstract = {Streaming data processing has been gaining attention due to its application into a wide range of scenarios. To serve the booming demands of streaming data processing, many computation engines have been developed. However, there is still a lack of real-world benchmarks that would be helpful when choosing the most appropriate platform for serving real-time streaming needs. In order to address this problem, we developed a streaming benchmark for three representative computation engines: Flink, Storm and Spark Streaming. Instead of testing speed-of-light event processing, we construct a full data pipeline using Kafka and Redis in order to more closely mimic the real-world production scenarios. Based on our experiments, we provide a performance comparison of the three data engines in terms of 99th percentile latency and throughput for various configurations.},
author = {Chintapalli, Sanket and Dagit, Derek and Evans, Bobby and Farivar, Reza and Graves, Thomas and Holderbaugh, Mark and Liu, Zhuo and Nusbaum, Kyle and Patil, Kishorkumar and Peng, Boyang Jerry and Poulosky, Paul},
doi = {10.1109/IPDPSW.2016.138},
file = {:C$\backslash$:/Users/uzmar/OneDrive/Documentos/Mendeley Desktop/Chintapalli et al. - 2016 - Benchmarking streaming computation engines Storm, flink and spark streaming.pdf:pdf},
isbn = {9781509021406},
journal = {Proceedings - 2016 IEEE 30th International Parallel and Distributed Processing Symposium, IPDPS 2016},
keywords = {Benchmark,Flink,Low Latency,Spark,Storm,Streaming processing},
pages = {1789--1792},
publisher = {IEEE},
title = {{Benchmarking streaming computation engines: Storm, flink and spark streaming}},
year = {2016}
}
